{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a7ddd2-9cc0-4398-933a-93d517a2e899",
   "metadata": {},
   "source": [
    "This notebook analyses the downloaded repositories and extracts information about the workflows, jobs and steps. \n",
    "\n",
    "The notebook generates the following files: \n",
    "\n",
    " - all_repositories.csv: the file contains all the repositories that were considered during the data extraction process. It contains various data about the repositories, such as their name, main branch, creation and update dates, stars, forks, etc. \n",
    " - repositories.csv: the file contains all the repositories that have at least one associated workflow. It's a subset of the previous file.\n",
    " - workflows.csv: the file contains an entry for each of the workflow files we found in the repositories. For each workflow, it indicates the repository, the filename, the name of the workflow, the list of events that trigger it (including reusable workflows) and the number of jobs.\n",
    " - jobs.csv: the file contains an entry for each of the jobs in workflow files. For each job, we indicate the repository, the workflow file, the job id and name (if any), whether it corresponds to (i.e., use \"uses:\") another workflow and which one and the number of steps.\n",
    " - steps.csv: the file contains an entry for each steps we found in jobs. For each step, we report on the repository, the workflow file, the job id, step name (if any), step position in the job, and the name of the action (i.e., the \"uses:\" field) if any, the number of lines in the \"run:\" field, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70349ce7-ce18-4a24-89ca-4025d37e306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ruamel.yaml as yaml\n",
    "from tqdm import tqdm \n",
    "\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1810e42d-dd5a-495b-8c53-6fca1c6a7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to repositories\n",
    "REPO_DIR = Path('/data/ghactions')\n",
    "\n",
    "# Path to data folder\n",
    "DATA_DIR = Path('../data/')\n",
    "\n",
    "# Number of parallel jobs\n",
    "WORKERS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656cd745-519b-41a3-a909-256f17cef50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial list of repositories. We will scan the REPO_DIR folder to see which ones \n",
    "# were effectively extracted and have a non-empty .github/workflows/ folder\n",
    "\n",
    "FIELDS = {\n",
    "    'Name': 'repository',\n",
    "    'Default Branch': 'branch',\n",
    "    'Main Language': 'language',\n",
    "    'Created At': 'created',\n",
    "    'Last Commit': 'updated',\n",
    "    'Last Commit SHA': 'commit',\n",
    "    'Stargazers': 'stars',\n",
    "    'Watchers': 'watchers',\n",
    "    'Forks': 'forks',\n",
    "    'Size': 'size',\n",
    "    'Branches': 'branches',\n",
    "    'Commits': 'commits',\n",
    "    'Contributors': 'contributors',\n",
    "    'Total Issues': 'issues',\n",
    "    'Total Pull Requests': 'prs',\n",
    "}\n",
    "\n",
    "df_input = (\n",
    "    pd.read_csv('../data-raw/repositories.csv')\n",
    "    [FIELDS.keys()]\n",
    "    .rename(columns=FIELDS)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fd0c5-2a91-43ea-b1fe-3b81832f8ace",
   "metadata": {},
   "source": [
    "Let's define a function that will extract the \"interesting parts\" of the yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d972211-0277-48bf-b793-5060575afaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_workflow(path):\n",
    "    \"\"\"\n",
    "    Given a path to a workflow file, extract parts of its content and return a dictionary mimicking the parts\n",
    "    of the y(a)ml file that are of interest (see documentation in this notebook for more details). \n",
    "    \"\"\"\n",
    "    output = dict()\n",
    "    with open(path) as f: \n",
    "        workflow = yaml.round_trip_load(f)\n",
    "    \n",
    "    if workflow is None:\n",
    "        return output\n",
    "    \n",
    "    # Name of the workflow\n",
    "    output['name'] = workflow.get('name')\n",
    "    \n",
    "    # List of events that trigger the workflow\n",
    "    if isinstance(workflow['on'], str):\n",
    "        output['events'] = [workflow.get('on')]\n",
    "    elif isinstance(workflow['on'], list):\n",
    "        output['events'] = list(workflow['on'])\n",
    "    elif isinstance(workflow['on'], dict):\n",
    "        output['events'] = list(workflow['on'].keys())\n",
    "    else:\n",
    "        assert False, f'Unsupported type {type(workflow.get(\"on\"))} for workflow.on field'\n",
    "        \n",
    "    # List of jobs\n",
    "    jobs = workflow.get('jobs', dict())\n",
    "    output['jobs'] = extract_jobs(jobs)\n",
    "        \n",
    "    return output\n",
    "    \n",
    "    \n",
    "def extract_jobs(jobs):\n",
    "    output = dict()\n",
    "    \n",
    "    for id, job in jobs.items():\n",
    "        output[id] = dict()\n",
    "        \n",
    "        output[id]['name'] = job.get('name')\n",
    "        output[id]['uses'] = job.get('uses')\n",
    "        output[id]['steps'] = extract_steps(job.get('steps', []))\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_steps(steps):\n",
    "    output = []\n",
    "    \n",
    "    for i, step in enumerate(steps):\n",
    "        item = dict()\n",
    "        \n",
    "        item['name'] = step.get('name')\n",
    "        item['position'] = i + 1\n",
    "        item['uses'] = step.get('uses')\n",
    "        _run = step.get('run', None)\n",
    "        item['run'] = len(_run.split('\\n')) if _run is not None else 0\n",
    "        output.append(item)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4efb9-00e4-4b44-9d6c-1422ebd8f0e9",
   "metadata": {},
   "source": [
    "Let's define a thin wrapper to handle outputs and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9de0b35-f263-4936-a3c4-a26b0138d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job(repository):\n",
    "    # Look if repository exists\n",
    "    path = REPO_DIR / repository.replace('/', '---') / '.github/workflows'\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    # Look for workflow files\n",
    "    workflows = dict()\n",
    "    for file in path.iterdir():\n",
    "        if file.suffix in ['.yaml', '.yml']:\n",
    "            try:\n",
    "                workflows[file.name] = extract_workflow(file)\n",
    "            except Exception as e:\n",
    "                workflows[file.name] = e\n",
    "    return workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053a29e6-2284-4d3e-b0bd-4a5b7fc5fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 69147/69147 [00:53<00:00, 1282.03it/s]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "inputs = list(df_input.itertuples())\n",
    "\n",
    "with Pool(processes=WORKERS) as pool:\n",
    "    jobs = pool.imap(job, [x.repository for x in inputs])\n",
    "    for repo, result in tqdm(zip(inputs, jobs), total=len(inputs)):\n",
    "        output.append((repo, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4940b3-83f7-41d4-bdc0-b5f2c59c5983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29828 repositories with workflows out of 69147.\n",
      "There were 121 errors during the process.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len([x for x in output if x[1] is not None])} repositories with workflows out of {len(output)}.')\n",
    "_ = [x for x in output if x[1] is not None and any([isinstance(w, Exception) for w in x[1].values()])]\n",
    "print(f'There were {len(_)} errors during the process.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc8bef-61e8-4e6d-9a1c-74cfc9a25a32",
   "metadata": {},
   "source": [
    "Now we can export these results as csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a1a15f-37f0-4879-9262-b6a9bdf0119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store data (they will be converted to DataFrames afterward)\n",
    "m_repositories = []\n",
    "m_workflows = []\n",
    "m_jobs = []\n",
    "m_steps = []\n",
    "\n",
    "for repository, workflows in output:\n",
    "    # Skip if there is no parsed workflow\n",
    "    if workflows is None or all([isinstance(e, Exception) for e in workflows.values()]):\n",
    "        continue\n",
    "    \n",
    "    m_repositories.append(tuple([\n",
    "        getattr(repository, field) for field in FIELDS.values()\n",
    "    ]))\n",
    "    \n",
    "    for filename, workflow in workflows.items():\n",
    "        if isinstance(workflow, Exception):\n",
    "            continue\n",
    "            \n",
    "        m_workflows.append((\n",
    "            repository.repository,\n",
    "            filename,\n",
    "            workflow.get('name'),\n",
    "            ', '.join(workflow.get('events', [])),\n",
    "            len(workflow.get('jobs', [])),\n",
    "        ))\n",
    "        \n",
    "        for job_id, job in workflow.get('jobs', dict()).items():\n",
    "            m_jobs.append((\n",
    "                repository.repository,\n",
    "                filename,\n",
    "                job_id,\n",
    "                job.get('name'),\n",
    "                job.get('uses'),\n",
    "                len(job.get('steps', [])),\n",
    "            ))\n",
    "            \n",
    "            for step in job.get('steps', []):\n",
    "                m_steps.append((\n",
    "                    repository.repository,\n",
    "                    filename,\n",
    "                    job_id,\n",
    "                    step.get('name'),\n",
    "                    step['position'],\n",
    "                    step.get('uses'),\n",
    "                    step['run'],\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901551a7-f3a5-4a07-939b-b7f623187b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29778, 70278, 108500, 576352)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_repositories), len(m_workflows), len(m_jobs), len(m_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5629e6-67a7-45f2-8d0d-3d684d903d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repositories = (\n",
    "    pd.DataFrame(m_repositories, columns=FIELDS.values())\n",
    "    .set_index('repository')\n",
    ")\n",
    "df_workflows = (\n",
    "    pd.DataFrame(m_workflows, columns=['repository', 'filename', 'name', 'events', 'jobs'])\n",
    "    .set_index(['repository', 'filename'])\n",
    ")\n",
    "df_jobs = (\n",
    "    pd.DataFrame(m_jobs, columns=['repository', 'filename', 'id', 'name', 'uses', 'steps'])\n",
    "    .set_index(['repository', 'filename', 'id'])\n",
    ")\n",
    "df_steps = (\n",
    "    pd.DataFrame(m_steps, columns=['repository', 'filename', 'job', 'name', 'pos', 'uses', 'run'])\n",
    "    .set_index(['repository', 'filename', 'job', 'pos'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0223b52-3169-4d1e-9580-92df0a70d85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29778, 70278, 108500, 576352)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_repositories), len(df_workflows), len(df_jobs), len(df_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0684765-037e-4596-a191-b03d8f9518c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input[FIELDS.values()].set_index('repository').to_csv(DATA_DIR / 'all_repositories.csv.gz', compression='gzip')\n",
    "df_repositories.to_csv(DATA_DIR / 'repositories.csv.gz', compression='gzip')\n",
    "df_workflows.to_csv(DATA_DIR / 'workflows.csv.gz', compression='gzip')\n",
    "df_jobs.to_csv(DATA_DIR / 'jobs.csv.gz', compression='gzip')\n",
    "df_steps.to_csv(DATA_DIR / 'steps.csv.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
